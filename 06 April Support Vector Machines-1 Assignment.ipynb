{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac9d1a8-86c4-44d9-af8c-0e6396246c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "\n",
    "\n",
    "The mathematical formula for a linear Support Vector Machine (SVM) can be expressed as follows:\n",
    "\n",
    "Given a dataset with feature vectors X_i in a D-dimensional space and corresponding binary class labels y_i (+1 or -1 for two-class classification):\n",
    "\n",
    "The decision function for a linear SVM is defined as:\n",
    "\n",
    "f(x) = w · x + b\n",
    "\n",
    "Here,\n",
    "\n",
    "f(x) is the decision function that predicts the class label for a given input vector x.\n",
    "w is the weight vector, which determines the direction of the decision boundary.\n",
    "x is the input feature vector.\n",
    "b is the bias term, which shifts the decision boundary away from the origin.\n",
    "The goal of training a linear SVM is to find the optimal values for the weight vector w and the bias term b that maximize the margin between the two classes while minimizing classification errors.\n",
    "\n",
    "The optimization problem can be expressed as:\n",
    "\n",
    "Minimize: 1/2 ||w||^2\n",
    "\n",
    "Subject to: y_i(w · x_i + b) ≥ 1 for all data points (i)\n",
    "\n",
    "Here, ||w|| represents the Euclidean norm of the weight vector w, and the constraint ensures that all data points are correctly classified and are at least a margin of 1 away from the decision boundary.\n",
    "\n",
    "This optimization problem can be solved using various techniques, such as the quadratic programming solver, to find the optimal values for w and b.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q2. What is the objective function of a linear SVM?\n",
    "\n",
    "The objective function of a linear Support Vector Machine (SVM) is a mathematical expression that defines the optimization problem that the SVM aims to solve. The primary goal of this objective function is to find the optimal values for the weight vector (w) and the bias term (b) in the linear SVM model in a way that maximizes the margin between the two classes while minimizing classification errors. Here is the formal representation of the objective function for a linear SVM:\n",
    "\n",
    "Minimize: 1/2 ||w||^2\n",
    "\n",
    "Subject to: y_i(w · x_i + b) ≥ 1 for all data points (i)\n",
    "\n",
    "In this objective function:\n",
    "\n",
    "||w|| represents the Euclidean norm (L2 norm) of the weight vector w. It is a measure of the magnitude or length of the weight vector.\n",
    "\n",
    "The goal is to minimize 1/2 ||w||^2. This term is often referred to as the \"regularization term\" or the \"L2 regularization term.\" It encourages finding a solution where the weight vector w is as small as possible. This, in turn, helps in achieving a wider margin between the classes, as larger values of w will result in smaller margins.\n",
    "\n",
    "The subject to constraint specifies that for each data point (x_i), the product of the class label (y_i) and the decision function (w · x_i + b) must be greater than or equal to 1. This constraint ensures that all data points are correctly classified and are at least a margin of 1 away from the decision boundary.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q3. What is the kernel trick in SVM?\n",
    "\n",
    "The kernel trick is a fundamental concept in Support Vector Machines (SVMs) that allows SVMs to handle non-linearly separable data by implicitly mapping the input data into a higher-dimensional feature space. It enables linear classifiers, such as linear SVMs, to work effectively in scenarios where a linear decision boundary cannot separate the classes in the original input space.\n",
    "\n",
    "Here's how the kernel trick works:\n",
    "\n",
    "Original Input Space: In the original input space, the data may not be linearly separable, meaning a simple straight line or hyperplane cannot effectively separate the different classes.\n",
    "\n",
    "Mapping to a Higher-Dimensional Space: The kernel trick involves mapping the data points from the original input space to a higher-dimensional feature space using a mathematical function called a \"kernel.\" This mapping is often non-linear and can transform the data into a space where it becomes linearly separable.\n",
    "\n",
    "The kernel function K(x, x') calculates the inner product (dot product) of two data points x and x' in the higher-dimensional space. Different types of kernel functions can be used, including:\n",
    "\n",
    "Linear Kernel: K(x, x') = x · x' (no mapping, remains in the original space).\n",
    "Polynomial Kernel: K(x, x') = (γ * (x · x') + r)^d, where γ, r, and d are hyperparameters.\n",
    "Radial Basis Function (RBF) Kernel (Gaussian Kernel): K(x, x') = exp(-γ * ||x - x'||^2), where γ is a hyperparameter.\n",
    "Linear Separation in the Feature Space: In the higher-dimensional feature space, the transformed data may now be linearly separable. This means that a hyperplane can effectively separate the classes, even though it corresponds to a non-linear decision boundary in the original input space.\n",
    "\n",
    "Solving the Linear SVM: Once the data is transformed, you can use a standard linear SVM to find the optimal hyperplane that separates the classes in the feature space.\n",
    "\n",
    "Predictions in the Original Space: When making predictions for new data points in the original input space, the kernel trick allows you to apply the learned linear decision boundary from the feature space without explicitly mapping the new data into the feature space. This is achieved through the use of kernel functions, which compute the dot product between the new data point and the support vectors in the feature space.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q4. What is the role of support vectors in SVM Explain with example\n",
    "\n",
    "\n",
    "Support vectors play a crucial role in Support Vector Machines (SVMs). They are the data points from the training dataset that are closest to the decision boundary (the hyperplane) and have a non-zero contribution to defining the decision boundary. Support vectors are instrumental in defining the margin, which is the distance between the decision boundary and the nearest support vectors. Their role can be explained using an example:\n",
    "\n",
    "Example: Binary Classification with Linear SVM\n",
    "\n",
    "Suppose you have a binary classification problem with two classes, A and B, and your goal is to find a linear decision boundary that separates these classes.\n",
    "\n",
    "Here's a simplified dataset in a 2D feature space:\n",
    "\n",
    "                      \n",
    "    Class A (positive):   O    O      O\n",
    "Class B (negative):      O    O    O\n",
    "\n",
    "                      \n",
    " In this example, the circles represent data points. Class A is represented by circles with an 'O,' and Class B is represented by circles with an 'X.' The goal is to find a straight line (the decision boundary) that separates the two classes.\n",
    "\n",
    "Linear SVM Training: When you train a linear SVM on this dataset, it identifies the optimal hyperplane (decision boundary) that maximizes the margin between the classes. The margin is the distance between the decision boundary and the nearest data points.\n",
    "\n",
    "Support Vectors: The support vectors are the data points that are closest to the decision boundary and directly influence its position. In this case, they are the points on or near the decision boundary.\n",
    "\n",
    "The support vectors from Class A are the three circles on the left side.\n",
    "The support vectors from Class B are the three circles on the right side.\n",
    "These points are crucial because if you move the decision boundary even slightly, it will affect these points. They essentially \"support\" the position of the decision boundary.\n",
    "\n",
    "Margin: The margin of the SVM is defined as the distance between the decision boundary and the nearest support vector. In this case, it's the distance between the decision boundary and one of the circles (a support vector) from Class A.\n",
    "\n",
    "Margin = Distance between decision boundary and support vector\n",
    "Classification: During the testing or prediction phase, when you encounter a new data point, the SVM determines which side of the decision boundary it falls on. If the data point is closer to one class of support vectors, it's classified as belonging to that class.\n",
    "\n",
    "The key role of support vectors can be summarized as follows:\n",
    "\n",
    "Support vectors are the critical data points that define the position and orientation of the decision boundary.\n",
    "They are the closest data points to the decision boundary and have a non-zero margin.\n",
    "The margin is maximized by selecting the support vectors as reference points.\n",
    "The SVM uses support vectors to make predictions for new data points by determining their position relative to the decision boundary.   \n",
    "                      \n",
    "                      \n",
    "\n",
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?\n",
    "\n",
    "                      \n",
    "To illustrate the concepts of Hyperplane, Marginal Plane, Soft Margin, and Hard Margin in Support Vector Machines (SVM), let's consider a simple two-dimensional binary classification problem with two classes: Class A (represented by circles) and Class B (represented by crosses). We'll use example graphs to visualize these concepts.\n",
    "\n",
    "1. Hyperplane:\n",
    "\n",
    "The hyperplane is the decision boundary that separates the two classes in the feature space. In a two-dimensional space, it's a line. The hyperplane is defined by the SVM to maximize the margin between the classes.\n",
    "\n",
    "Example Graph:\n",
    "\n",
    "         |        +\n",
    "     |      /|\n",
    "     |    /  |\n",
    "     |  /    |\n",
    "     |/      |   \n",
    "     +-------+ (Hyperplane)\n",
    "     |       |\n",
    "     |       |\n",
    "     |       |\n",
    "     |       |\n",
    "     |       |\n",
    "Class A    Class B\n",
    "\n",
    "                      \n",
    "In this graph, the hyperplane is the straight line that separates Class A from Class B.\n",
    "\n",
    "2. Marginal Plane:\n",
    "\n",
    "The marginal plane refers to the planes parallel to the hyperplane that are at a certain distance from it. These planes help define the margin. The data points closest to these marginal planes are called support vectors.\n",
    "\n",
    "Example Graph:\n",
    "\n",
    "        |        +\n",
    "     |        |\n",
    "     |        |\n",
    "     |        |\n",
    "     |        |\n",
    "     |        |\n",
    "     |        |\n",
    "     |        |\n",
    "     |       /|   \n",
    "     |     /  |\n",
    "     |   /    |\n",
    "     | /      |   \n",
    "     +-------+ (Hyperplane)\n",
    "     |       |\n",
    "     |       |\n",
    "Class A    Class B\n",
    "\n",
    "                      \n",
    "In this graph, the dashed lines represent the marginal planes parallel to the hyperplane. The data points near these planes (the '+' symbols) are support vectors.\n",
    "\n",
    "3. Soft Margin:\n",
    "\n",
    "Soft margin allows for some misclassification in the training data to find a more flexible decision boundary. It introduces a margin of error, allowing some data points to cross the margin boundary or even the hyperplane. This is especially useful when dealing with noisy or overlapping data.\n",
    "\n",
    "Example Graph:\n",
    "\n",
    "      |        +\n",
    "     |      / |\n",
    "     |    /   |\n",
    "     |  /     |\n",
    "     |/       |   \n",
    "     +-------+ (Hyperplane with Soft Margin)\n",
    "     |       |\n",
    "     |       |\n",
    "     |       |\n",
    "     |       |\n",
    "     |       |\n",
    "     |       |\n",
    "Class A    Class B\n",
    "\n",
    "  In this graph, you can see that a few data points from Class A cross the margin boundary but are still correctly classified.\n",
    "\n",
    "4. Hard Margin:\n",
    "\n",
    "Hard margin SVM does not allow any misclassification in the training data. It requires a clear separation between the classes, and the margin is maximized with no data points inside or crossing it. Hard margin SVMs are sensitive to outliers and noise.\n",
    "\n",
    "Example Graph: \n",
    "                      \n",
    "         |        +\n",
    "     |        |\n",
    "     |        |\n",
    "     |        |\n",
    "     |        |\n",
    "     |        |\n",
    "     |        |\n",
    "     |        |\n",
    "     |        |   \n",
    "     |        |\n",
    "     |        |\n",
    "     |        |\n",
    "     +-------+ (Hyperplane with Hard Margin)\n",
    "     |       |\n",
    "Class A    Class B\n",
    "\n",
    "  In this graph, there are no data points inside or crossing the margin boundary, representing a hard margin SVM.   \n",
    "\n",
    "                      \n",
    "Q6. SVM Implementation through Iris dataset.\n",
    "\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Using only the first two features for simplicity\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear SVM classifier\n",
    "svm_classifier = SVC(kernel='linear', C=1.0)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing data\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of scikit-learn SVM: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Plot the decision boundaries\n",
    "def plot_decision_boundary(X, y, classifier, ax):\n",
    "    h = 0.02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolor='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot decision boundaries for the SVM classifier\n",
    "for i, C in enumerate([1, 100]):\n",
    "    svc = SVC(kernel='linear', C=C)\n",
    "    svc.fit(X_train, y_train)\n",
    "    plot_decision_boundary(X_train, y_train, svc, axes[i])\n",
    "    axes[i].set_title(f\"SVM (C = {C})\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "Now, let's implement a simple linear SVM from scratch and compare its performance:\n",
    "\n",
    "                      \n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Using only the first two features for simplicity\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Implement a simple linear SVM from scratch\n",
    "class LinearSVM:\n",
    "    def __init__(self, learning_rate=0.01, num_epochs=1000, C=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.C = C\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for i in range(n_samples):\n",
    "                if y[i] * (np.dot(X[i], self.w) - self.b) >= 1:\n",
    "                    self.w -= self.learning_rate * (2 * self.C * self.w)\n",
    "                else:\n",
    "                    self.w -= self.learning_rate * (2 * self.C * self.w - np.dot(X[i], y[i]))\n",
    "                    self.b -= self.learning_rate * y[i]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.w) - self.b)\n",
    "\n",
    "# Create and train the LinearSVM classifier from scratch\n",
    "svm_scratch = LinearSVM(learning_rate=0.01, num_epochs=1000, C=1.0)\n",
    "svm_scratch.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing data\n",
    "y_pred_scratch = svm_scratch.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy_scratch = accuracy_score(y_test, y_pred_scratch)\n",
    "print(f\"Accuracy of SVM from scratch: {accuracy_scratch * 100:.2f}%\")\n",
    "\n",
    "                      \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
